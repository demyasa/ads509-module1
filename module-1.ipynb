{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Module 1 Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import relevant libaries\n",
    "\n",
    "import os\n",
    "import datetime\n",
    "import re\n",
    "\n",
    "# Libraries for lyrics scrape section\n",
    "\n",
    "import requests\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import defaultdict, Counter\n",
    "import random\n",
    "\n",
    "# Additional libraries\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 1: Pulling and Storing Lyric Links"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Choose two musical artists for your project. Both should have at least twenty songs on their artist page on AZLyrics.*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adele & Eminem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Note on Rate Limiting*\n",
    "\n",
    "Whenever you call `requests.get` to retrieve a page, put a `time.sleep(5 + 10*random.random())` on the next line. This will help you not to get blocked. If you do get blocked, which you can identify if the returned pages are not correct, just request a lyrics page through your browser. You'll be asked to perform a CAPTCHA and then your requests should start working again.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store artist name and link in dictionary\n",
    "\n",
    "artists = {\n",
    "    \"adele\":\"https://www.azlyrics.com/a/adele.html\",\n",
    "    \"eminem\":\"https://www.azlyrics.com/e/eminem.html\"\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Take a look at the `robots.txt` page on www.azlyrics.com. Is the scraping we are about to do allowed or disallowed by this page? How do you know?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A: Glancing at the robots.txt page provides the following information:\n",
    "\n",
    "```\n",
    "        User-agent: *\n",
    "        Disallow: /lyricsdb/\n",
    "        Disallow: /song/\n",
    "        Allow: /\n",
    "\n",
    "        User-agent: 008\n",
    "        Disallow: /\n",
    "```\n",
    "\n",
    "Based on the `Disallow` keys, the only endpoints that are not allowed to be scraped are those that include either `/lyricsdb` and `/song`. Since the endpoints being hit in this project are artist names and lyrics of specific songs (not the lyricsdb directly nor the audio file of the song itself), every other endpoint should be fair game unless the `User-agent` is `008`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Scrape Link List*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary to hold lyric links\n",
    "\n",
    "lyrics_pages = defaultdict(list)\n",
    "\n",
    "for artist, artist_page in artists.items() :\n",
    "\n",
    "    # Request the page and sleep\n",
    "\n",
    "    r = requests.get(artist_page)\n",
    "    time.sleep(5 + 10*random.random())\n",
    "\n",
    "    # Convert HTML response to text using bs4\n",
    "\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "    # Extract all song divs containing links to lyrics\n",
    "\n",
    "    link_divs = soup.find_all('div',attrs={'class':'listalbum-item'})\n",
    "\n",
    "    # Extract links from divs and store in list\n",
    "\n",
    "    link_list = []\n",
    "    for div in link_divs:\n",
    "        href = div.find('a', href=True)\n",
    "        link = f\"https://www.azlyrics.com{href['href']}\"\n",
    "        link_list.append(link)\n",
    "\n",
    "    # Store in `lyrics_pages` with key = artist & value = link_list\n",
    "\n",
    "    lyrics_pages[artist] = link_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check song count\n",
    "\n",
    "for artist, lp in lyrics_pages.items():\n",
    "    assert(len(set(lp)) > 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 393 listed song lyrics for Adele\n",
      "For adele we have 71.\n",
      "The full pull will take for this artist will take 0.2 hours.\n",
      "There are 393 listed song lyrics for Eminem\n",
      "For eminem we have 412.\n",
      "The full pull will take for this artist will take 1.14 hours.\n"
     ]
    }
   ],
   "source": [
    "# Calculate time to pull all lyrics with delay of `5 + 10*random.random()` seconds \n",
    "\n",
    "for artist, links in lyrics_pages.items() :\n",
    "    print(f\"For {artist.capitalize()} we have {len(links)} song lyric links.\")\n",
    "    print(f\"The full pull for this artist will take {round(len(links)*10/3600,2)} hours.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 2: Pulling and Storing Lyric Pages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for creating filename based on song title and writing to text file using that name\n",
    "\n",
    "def generate_filename_from_link(link) :\n",
    "    \n",
    "    if not link :\n",
    "        return None\n",
    "    \n",
    "    # Drop the http or https and the html\n",
    "\n",
    "    name = link.replace(\"https\",\"\").replace(\"http\",\"\")\n",
    "    name = link.replace(\".html\",\"\")\n",
    "    name = name.replace(\"/lyrics/\",\"\")\n",
    "    \n",
    "    # Replace useless characters with UNDERSCORE\n",
    "\n",
    "    name = name.replace(\"://\",\"\").replace(\".\",\"_\").replace(\"/\",\"_\")\n",
    "    \n",
    "    # Tack on .txt\n",
    "\n",
    "    name = name + \".txt\"\n",
    "    \n",
    "    return(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for checking if lyrics folder exists\n",
    "# Using shutil.rmtree to remove and create a new one if it does\n",
    "\n",
    "if os.path.isdir(\"lyrics\") : \n",
    "    shutil.rmtree(\"lyrics/\")\n",
    "\n",
    "os.mkdir(\"lyrics\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Scrape Page Lyrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_stub = \"https://www.azlyrics.com\" \n",
    "start = time.time()\n",
    "\n",
    "total_pages = 0\n",
    "\n",
    "for artist, links in lyrics_pages.items() :\n",
    "    \n",
    "    # 1. Build a subfolder for the artist\n",
    "\n",
    "    if os.path.isdir(f\"lyrics/{artist}\"): \n",
    "        shutil.rmtree(f\"lyrics/{artist}\")\n",
    "    os.mkdir(f\"lyrics/{artist}\")\n",
    "\n",
    "    # 2. Iterate over the lyrics pages (30 pages per artist)\n",
    "\n",
    "    for link in links[0:30] :\n",
    "    \n",
    "        # 3. Request the page and sleep\n",
    "\n",
    "        r = requests.get(link)\n",
    "        time.sleep(5 + 10*random.random())\n",
    "\n",
    "        # Convert HTML response to text using bs4\n",
    "\n",
    "        soup = BeautifulSoup(r.text, 'html.parser')\n",
    "\n",
    "        # 4. Extract title and lyrics from page\n",
    "\n",
    "        ringtone = soup.find('div', attrs = {'class' : 'ringtone'})\n",
    "        title = soup.select_one('b:nth-child(5)').text\n",
    "        lyrics = ringtone.find_next_sibling(\"div\").text\n",
    "\n",
    "        # 5. Write out title and lyrics split by two returns\n",
    "\n",
    "        song_lyrics = f\"{title}\\n\\n{lyrics}\"\n",
    "\n",
    "        # Using generate_filename_from_url results in ugly file names for me\n",
    "\n",
    "            # fname = generate_filename_from_link(link)\n",
    "            # Example Output: httpswww_azlyrics_comadele_daydreamer.txt\n",
    "\n",
    "        # Below seems like a cleaner approach\n",
    "        \n",
    "        file_title = title.replace('\"','').replace(' ', '_')\n",
    "        fname = f\"{file_title}.txt\"\n",
    "\n",
    "        # Write lyrics file to appropriate directory\n",
    "\n",
    "        f = open(f\"lyrics/{artist}/{fname}\", \"w\")\n",
    "        f.write(song_lyrics)\n",
    "        f.close()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "### Checking Lyrics \n",
    "\n",
    "The output from your lyrics scrape should be stored in files located in this path from the directory:\n",
    "`/lyrics/[Artist Name]/[filename from URL]`. This code summarizes the information at a high level to help the instructor evaluate your work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple word extractor from Peter Norvig: https://norvig.com/spell-correct.html\n",
    "def words(text): \n",
    "    return re.findall(r'\\w+', text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Eminem we have 29 files.\n",
      "For Eminem we have roughly 18516 words, 3021 are unique.\n",
      "For Adele we have 30 files.\n",
      "For Adele we have roughly 9299 words, 986 are unique.\n"
     ]
    }
   ],
   "source": [
    "# Minor alterations made here so the evaluation works\n",
    "\n",
    "artist_folders = os.listdir(\"lyrics/\")\n",
    "artist_folders = [f for f in artist_folders if os.path.isdir(\"lyrics/\" + f)]\n",
    "\n",
    "for artist in artist_folders : \n",
    "    artist_files = os.listdir(\"lyrics/\" + artist)\n",
    "    artist_files = [f for f in artist_files if 'txt' in f or 'csv' in f or 'tsv' in f]\n",
    "\n",
    "    print(f\"For {artist.capitalize()} we have {len(artist_files)} files.\")\n",
    "\n",
    "    artist_words = []\n",
    "\n",
    "    for f_name in artist_files : \n",
    "        # with open(\"lyrics/\" + artist + \"/\" + f_name) as infile : \n",
    "        with open(f\"lyrics/{artist}/{f_name}\", \"r\") as infile:\n",
    "            artist_words.extend(words(infile.read()))\n",
    "\n",
    "    print(f\"For {artist.capitalize()} we have roughly {len(artist_words)} words, {len(set(artist_words))} are unique.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
